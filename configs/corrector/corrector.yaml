# This is the configuration for training the Corrector model.

# Global seed for reproducibility
global_seed: 0

# Corrector Model parameters
# We define the model we are actually training here.
corrector_model:
  target: RandAR.model.corrector.LinearCorrector
  params:
    num_ar_layers_for_input: 1
    dim: 1024
    hidden_dim: [4096, 4096]
    n_head: 16
    n_layer: 12 # A reasonable starting point, can be tuned
    dropout: 0.1
    cls_token_num: 1
    block_size: 256
    vocab_size: 16384
    rope_base: 10000

    num_transformer_layers: 2
    num_head: 8

# Training parameters
training_params:
  max_iters: 5000
  global_batch_size: 512
  perturb_mode: "num" # "ratio" or "num"
  perturb_ratio: 0.3 # Key hyperparameter for corrector training
  perturb_num: 1

# Optimization parameters
optimizer:
  target: torch.optim.AdamW
  lr: 1.0e-4
  weight_decay: 0.05
  betas: [0.9, 0.95]
  max_grad_norm: 1.0
  skip_grad_norm: 100.0 # if grad norm > this, skip optimizer step
  skip_grad_iter: 1000 # always step for the first N iters

# Learning rate scheduler
lr_scheduler:
  type: "cosine"
  warm_up_iters: 2000
  min_lr_ratio: 0.1
  num_cycles: 1

# Accelerator configuration for mixed precision and gradient accumulation
accelerator:
  mixed_precision: "bf16"
  gradient_accumulation_steps: 1
  log_with: "wandb" 